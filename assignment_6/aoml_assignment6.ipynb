{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9243,"sourceType":"datasetVersion","datasetId":2243}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#linear using grid search\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\n\n# Load dataset\ntrain_df = pd.read_csv('/kaggle/input/fashionmnist/fashion-mnist_train.csv')\ntest_df = pd.read_csv('/kaggle/input/fashionmnist/fashion-mnist_test.csv')\n\n# Extract features and labels\ny_train = train_df.iloc[:, 0].values  # Labels (0-9)\nX_train = train_df.iloc[:, 1:].values  # Features (pixel values)\ny_test = test_df.iloc[:, 0].values\nX_test = test_df.iloc[:, 1:].values\n\n# Standardize features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Split training data for validation\nX_train_sub, X_val, y_train_sub, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Define SVM model and hyperparameter grid\nparam_grid = {'C': [0.01, 0.1, 1, 10, 100]}\nsvm = SVC(kernel='linear')\n\n# Grid search with cross-validation\ngrid_search = GridSearchCV(svm, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\ngrid_search.fit(X_train_sub, y_train_sub)\n\n# Best C value\nbest_C = grid_search.best_params_['C']\nprint(f\"Best C: {best_C}\")\n\n# Train final model with best C\nbest_svm = SVC(kernel='linear', C=best_C)\nbest_svm.fit(X_train, y_train)\n\n# Evaluate on test set\ny_pred = best_svm.predict(X_test)\ntest_accuracy = accuracy_score(y_test, y_pred)\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#poly using grid search\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\n\n# Load dataset\ntrain_df = pd.read_csv('/kaggle/input/fashionmnist/fashion-mnist_train.csv')\ntest_df = pd.read_csv('/kaggle/input/fashionmnist/fashion-mnist_test.csv')\n\n# Extract features and labels\ny_train = train_df.iloc[:, 0].values  # Labels (0-9)\nX_train = train_df.iloc[:, 1:].values  # Features (pixel values)\ny_test = test_df.iloc[:, 0].values\nX_test = test_df.iloc[:, 1:].values\n\n# Standardize features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Split training data for validation\nX_train_sub, X_val, y_train_sub, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Define SVM model and hyperparameter grid\nparam_grid = {\n    'C': [0.01, 0.1, 1, 10, 100],  # Regularization parameter\n    'degree': [2, 3, 4, 5]  # Polynomial degrees to test\n}\nsvm = SVC(kernel='poly')\n\n# Grid search with cross-validation\ngrid_search = GridSearchCV(svm, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\ngrid_search.fit(X_train_sub, y_train_sub)\n\n# Best hyperparameters\nbest_C = grid_search.best_params_['C']\nbest_degree = grid_search.best_params_['degree']\nprint(f\"Best C: {best_C}, Best Polynomial Degree: {best_degree}\")\n\n# Train final model with best hyperparameters\nbest_svm = SVC(kernel='poly', C=best_C, degree=best_degree)\nbest_svm.fit(X_train, y_train)\n\n# Evaluate on test set\ny_pred = best_svm.predict(X_test)\ntest_accuracy = accuracy_score(y_test, y_pred)\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T17:58:48.063777Z","iopub.execute_input":"2025-03-08T17:58:48.064121Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#poly using random search\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.stats import uniform, randint  # Random distributions for hyperparameters\n\n# Load dataset\ntrain_df = pd.read_csv('/kaggle/input/fashionmnist/fashion-mnist_train.csv')\ntest_df = pd.read_csv('/kaggle/input/fashionmnist/fashion-mnist_test.csv')\n\n# Extract features and labels\ny_train = train_df.iloc[:, 0].values\nX_train = train_df.iloc[:, 1:].values\ny_test = test_df.iloc[:, 0].values\nX_test = test_df.iloc[:, 1:].values\n\n# Standardize features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Split training data for validation\nX_train_sub, X_val, y_train_sub, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Define SVM model and hyperparameter search space\nparam_dist = {\n    'C': uniform(0.01, 100),  # Randomly pick C between 0.01 and 100\n    'degree': randint(2, 6)  # Randomly choose polynomial degree from 2 to 5\n}\nsvm = SVC(kernel='poly')\n\n# Randomized Search with cross-validation\nrandom_search = RandomizedSearchCV(svm, param_dist, n_iter=10, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\nrandom_search.fit(X_train_sub, y_train_sub)\n\n# Best hyperparameters\nbest_C = random_search.best_params_['C']\nbest_degree = random_search.best_params_['degree']\nprint(f\"Best C: {best_C}, Best Polynomial Degree: {best_degree}\")\n\n# Train final model with best hyperparameters\nbest_svm = SVC(kernel='poly', C=best_C, degree=best_degree)\nbest_svm.fit(X_train, y_train)\n\n# Evaluate on test set\ny_pred = best_svm.predict(X_test)\ntest_accuracy = accuracy_score(y_test, y_pred)\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q cuml-cu11 --extra-index-url=https://pypi.nvidia.com","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#poly using rapids package with random search\n\nimport numpy as np\nimport pandas as pd\nfrom cuml.svm import SVC  # RAPIDS GPU-accelerated SVM\nfrom cuml.model_selection import train_test_split, RandomizedSearchCV\nfrom cuml.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nfrom scipy.stats import uniform, randint\n\n# Load dataset\ntrain_df = pd.read_csv('/kaggle/input/fashionmnist/fashion-mnist_train.csv')\ntest_df = pd.read_csv('/kaggle/input/fashionmnist/fashion-mnist_test.csv')\n\n# Extract features and labels\ny_train = train_df.iloc[:, 0].values\nX_train = train_df.iloc[:, 1:].values\ny_test = test_df.iloc[:, 0].values\nX_test = test_df.iloc[:, 1:].values\n\n# Standardize features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Split training data for validation\nX_train_sub, X_val, y_train_sub, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Define SVM model and hyperparameter search space\nparam_dist = {\n    'C': uniform(0.01, 100),\n    'degree': randint(2, 6)\n}\nsvm = SVC(kernel='poly')\n\n# Randomized Search with RAPIDS\nrandom_search = RandomizedSearchCV(svm, param_dist, n_iter=10, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\nrandom_search.fit(X_train_sub, y_train_sub)\n\n# Best hyperparameters\nbest_C = random_search.best_params_['C']\nbest_degree = random_search.best_params_['degree']\nprint(f\"Best C: {best_C}, Best Polynomial Degree: {best_degree}\")\n\n# Train final GPU-accelerated model with best hyperparameters\nbest_svm = SVC(kernel='poly', C=best_C, degree=best_degree)\nbest_svm.fit(X_train, y_train)\n\n# Evaluate on test set\ny_pred = best_svm.predict(X_test)\ntest_accuracy = accuracy_score(y_test, y_pred)\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# rbf with rapids and grid search\n\nimport numpy as np\nimport pandas as pd\nfrom cuml.svm import SVC  # RAPIDS GPU-accelerated SVM\nfrom cuml.model_selection import train_test_split, GridSearchCV\nfrom cuml.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\n\n# Load dataset\ntrain_df = pd.read_csv('/kaggle/input/fashionmnist/fashion-mnist_train.csv')\ntest_df = pd.read_csv('/kaggle/input/fashionmnist/fashion-mnist_test.csv')\n\n# Extract features and labels\ny_train = train_df.iloc[:, 0].values\nX_train = train_df.iloc[:, 1:].values\ny_test = test_df.iloc[:, 0].values\nX_test = test_df.iloc[:, 1:].values\n\n# Standardize features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Split training data for validation\nX_train_sub, X_val, y_train_sub, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Define SVM model and hyperparameter grid\nparam_grid = {\n    'C': [0.01, 0.1, 1, 10, 100],\n    'gamma': [0.0001, 0.001, 0.01, 0.1, 1]\n}\nsvm = SVC(kernel='rbf')\n\n# Grid search with RAPIDS\ngrid_search = GridSearchCV(svm, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\ngrid_search.fit(X_train_sub, y_train_sub)\n\n# Best hyperparameters\nbest_C = grid_search.best_params_['C']\nbest_gamma = grid_search.best_params_['gamma']\nprint(f\"Best C: {best_C}, Best Gamma: {best_gamma}\")\n\n# Train final GPU-accelerated model with best hyperparameters\nbest_svm = SVC(kernel='rbf', C=best_C, gamma=best_gamma)\nbest_svm.fit(X_train, y_train)\n\n# Evaluate on test set\ny_pred = best_svm.predict(X_test)\ntest_accuracy = accuracy_score(y_test, y_pred)\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}